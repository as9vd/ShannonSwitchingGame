{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334923f5-35e6-4cc3-9b2c-86693f029a40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Initial Shit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b48344-a4ab-49d8-afbb-1685708daac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67873cb1-19d5-43bf-aa75-24549407aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, row, col):\n",
    "        self.val = (row, col) # Value of node = its coordinates ((0, 0) = top-left/start, (m - 1, n - 1) = bottom-right/end).\n",
    "        self.adj_list = set()\n",
    "\n",
    "# create a list of edges for Game\n",
    "class Graph:\n",
    "    def __init__(self, num_rows, num_cols):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.nodes = self._create_nodes()\n",
    "        \n",
    "        self.edges = set()\n",
    "        for row in range(self.num_rows): # Kinda like initialising a 2D matrix. From the nodes generated, add the edges to the set.\n",
    "            for col in range(self.num_cols):\n",
    "                node = self.nodes[row][col]\n",
    "\n",
    "                adj_list = node.adj_list\n",
    "                for other_node in adj_list:\n",
    "                    self.edges.add((node, other_node))\n",
    "                    self.edges.add((other_node, node))\n",
    "        \n",
    "    def init_start_and_end(self):\n",
    "        self.start = self.nodes[0][0] # top-left\n",
    "        self.end = self.nodes[self.num_rows - 1][self.num_cols - 1] # bottom-right\n",
    "        \n",
    "    def _create_nodes(self):\n",
    "        nodes = [[Node(row, col) for col in range(self.num_cols)] for row in range(self.num_rows)] # create normal node for entire matrix\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                node = nodes[row][col] # this stuff below is where we initialise the neighbours.\n",
    "                if row > 0:\n",
    "                    node.adj_list.add(nodes[row - 1][col])  # Upper neighbour.\n",
    "                    nodes[row - 1][col].adj_list.add(node)\n",
    "                if row < self.num_rows - 1:\n",
    "                    node.adj_list.add(nodes[row + 1][col])  # Lower neighbour.\n",
    "                    nodes[row + 1][col].adj_list.add(node)\n",
    "                if col > 0:\n",
    "                    node.adj_list.add(nodes[row][col - 1])  # Left neighbour.\n",
    "                    nodes[row][col - 1].adj_list.add(node)\n",
    "                if col < self.num_cols - 1:\n",
    "                    node.adj_list.add(nodes[row][col + 1])  # Right neighbour.\n",
    "                    nodes[row][col + 1].adj_list.add(node)\n",
    "        return nodes\n",
    "        \n",
    "    def print_graph(self): # for debugging purposes\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                print(self.nodes[row][col].val)\n",
    "            print()\n",
    "            \n",
    "    def print_adjacencies(self): # for debugging purposes\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                print((row, col), [node.val for node in self.nodes[row][col].adj_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5db77a65-854c-4354-924e-fa9788548bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(3, 3)\n",
    "g.init_start_and_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e09ef-f820-4942-9b31-36538fafa819",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1984589-c521-49e3-ac2c-76e4c07d21d1",
   "metadata": {},
   "source": [
    "Rules:\n",
    "1. s is in top-left (0, 0), t is bottom-right (m - 1, n - 1).\n",
    "2. Fix-type player wants is to secure a path from s to t; to do this, the fix-type player secures an edge in the graph in each iteration.\n",
    "3. Cut-type player wants to disconnect s and t; to do this, the cut-type player deletes an unsecured edge in the graph.\n",
    "4. Game ends when there is a secured path from s to t (fix) or there are no paths between s and t (cut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18fcad75-6b09-43a0-98ff-e0aeb4f67d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix: check for a path (dfs)\n",
    "# cut: no more valid edges to choose\n",
    "import random\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.m = self.graph.num_rows\n",
    "        self.n = self.graph.num_cols\n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n # this is for the CUT player\n",
    "        self.secured = [] # this is what the FIX player chooses; nodes\n",
    "        self.secured_edges = [] # fix  \n",
    "        self.removed_edges = [] # cut\n",
    "        \n",
    "        # these are the remaining unsecured edges    \n",
    "        # i've used a set comprehension so it's easier to see\n",
    "        # need to ensure both directions of edges are deleted when work is done (e.g. both ((0, 0), (1, 0)) and ((1, 0), (0, 0))\n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n # this is for the CUT player\n",
    "        self.secured = [] # this is what the FIX player chooses; nodes\n",
    "        self.secured_edges = [] # fix  \n",
    "        self.removed_edges = [] # cut\n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    # Plays step-by-step. This is what we'll use for \"learning\".\n",
    "    def next_step_player(self):\n",
    "        if self.unsecured_count > 0:\n",
    "            if not self.end:\n",
    "                # 1. CUT player's turn\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    edge_to_cut = self.choose_edge_to_cut()\n",
    "                    self.cut(edge_to_cut)\n",
    "\n",
    "            if not self.end:\n",
    "                # 2. FIX player's turn\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    edge_to_fix = self.choose_edge_to_fix()\n",
    "                    self.fix(edge_to_fix)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "        else:\n",
    "            self.end = True\n",
    "        \n",
    "    # Plays the entire thing.\n",
    "    def play(self):\n",
    "        while self.unsecured_count > 0:\n",
    "            # 1. CUT player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "                break\n",
    "                \n",
    "            edge_to_cut = self.choose_edge_to_cut()\n",
    "            self.cut(edge_to_cut)\n",
    "        \n",
    "            # 2. FIX player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "                break\n",
    "\n",
    "            edge_to_fix = self.choose_edge_to_fix()\n",
    "            self.fix(edge_to_fix)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "                break\n",
    "            \n",
    "    def choose_edge_to_cut(self):\n",
    "        # Need to implement some strategy here. Return as a tuple of coordinates.\n",
    "        edge_to_cut = random.choice(list(self.remaining))\n",
    "        return edge_to_cut\n",
    "\n",
    "    def choose_edge_to_fix(self):\n",
    "        # Need to implement some strategy here. Return as a tuple of coordinates.\n",
    "        edge_to_fix = random.choice(list(self.remaining))\n",
    "        return edge_to_fix\n",
    "\n",
    "    def is_fix_path_complete(self): # this does BFS to check if there is a path from the start to the end.\n",
    "        visited = set()\n",
    "        stack = [(0, 0)]\n",
    "\n",
    "        while stack:\n",
    "            current_node = stack.pop()\n",
    "            if current_node == (self.m - 1, self.n - 1):\n",
    "                return True\n",
    "\n",
    "            for i in range(len(self.secured) - 1):\n",
    "                edge = (self.secured[i], self.secured[i + 1])\n",
    "                reverse_edge = (self.secured[i + 1], self.secured[i])\n",
    "\n",
    "                if (edge in self.secured_edges or reverse_edge in self.secured_edges) and current_node == self.secured[i]:\n",
    "                    next_node = self.secured[i + 1]\n",
    "                    if next_node not in visited:\n",
    "                        visited.add(next_node)\n",
    "                        stack.append(next_node)\n",
    "\n",
    "        return False\n",
    "\n",
    "    # 1. CUT player's function; removes unsecured edge in question (and its reverse).\n",
    "    # Ideally we don't check if the edge is in self.remaining (we just assume it is).\n",
    "    # But perhaps the choose function might fuck up.\n",
    "    def cut(self, edge):\n",
    "        # edge = ex: ((0, 0), (1,0))\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.removed_edges.append(edge)\n",
    "            self.unsecured_count -= 1\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "            self.unsecured_count -= 1\n",
    "\n",
    "    def fix(self, edge):\n",
    "        # edge = ex: ((0, 0), (1,0))\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.secured.append(edge[0])\n",
    "            self.secured.append(edge[1])\n",
    "            self.secured_edges.append(edge)\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "            \n",
    "    # Reward function\n",
    "    def get_reward(self):\n",
    "        if self.fix_win:\n",
    "            # Positive reward when the FIX player wins\n",
    "            reward = 1.0\n",
    "        elif self.end:\n",
    "            # Negative reward when the FIX player loses\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            # Intermediate reward for the ongoing game\n",
    "            reward = 0.0            \n",
    "\n",
    "        return reward\n",
    "            \n",
    "    def get_state(self):\n",
    "        # Define the state representation based on the game state.\n",
    "        secured_count = len(self.secured_edges)\n",
    "        remaining_count = int(len(self.remaining) / 2) # Because reverse edges are here too.\n",
    "        secured_edges = self.secured_edges\n",
    "        deleted_edges = self.removed_edges\n",
    "        remaining_edges = list(self.remaining) # Yet for this, we'll keep the reverse edges. Bit hypocritical, but fuck it.\n",
    "        \n",
    "        state = (secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count)\n",
    "    \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a1f9de38-38a8-49fe-a3e9-6be2c18cc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    g = Graph(4, 4)\n",
    "    g.init_start_and_end()\n",
    "    game = Game(g)\n",
    "    game.play()\n",
    "    if game.fix_win:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a336313-8b3f-453f-aa26-873ecc270009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([((0, 2), (1, 2)),\n",
       "  ((1, 1), (1, 0)),\n",
       "  ((1, 1), (1, 2)),\n",
       "  ((1, 3), (2, 3)),\n",
       "  ((3, 3), (2, 3)),\n",
       "  ((1, 2), (1, 3)),\n",
       "  ((2, 1), (1, 1)),\n",
       "  ((2, 2), (1, 2)),\n",
       "  ((0, 0), (1, 0))],\n",
       " {((0, 1), (1, 1)),\n",
       "  ((0, 2), (0, 3)),\n",
       "  ((0, 3), (0, 2)),\n",
       "  ((1, 1), (0, 1)),\n",
       "  ((2, 0), (3, 0)),\n",
       "  ((2, 1), (2, 2)),\n",
       "  ((2, 1), (3, 1)),\n",
       "  ((2, 2), (2, 1)),\n",
       "  ((3, 0), (2, 0)),\n",
       "  ((3, 1), (2, 1)),\n",
       "  ((3, 1), (3, 2)),\n",
       "  ((3, 2), (3, 1))})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.secured_edges, game.remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c6664-36db-4c35-8907-1e15c8872a5b",
   "metadata": {},
   "source": [
    "# Actual Shit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320b0b6-649b-488f-884a-33f691fd6101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Game (Updated to Include AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14ad57-e084-4a00-b726-97ae62021455",
   "metadata": {},
   "source": [
    "What I did here mate was renumber the graphs so it follows the following scheme (for a 3x3 graph, for example):\n",
    "\n",
    "![3x3 graph](https://i.gyazo.com/3743bdc7923224e488a15dafaca0373c.png)\n",
    "\n",
    "This had to be done so I could train the model better. I also started writing the outline of the model, agent, and trainer. They are not done, however. But, based on what I've seen/tested so far, it looks like the updated Graph and GameAI classes work as intended. Sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "ed3c2644-2c9e-4f73-8aea-83007c32aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, val):\n",
    "        self.val = val # Value of node = its position (left, right, zigzag, continue).\n",
    "        self.adj_list = set()\n",
    "\n",
    "# create a list of edges for Game\n",
    "class Graph:\n",
    "    def __init__(self, num_rows, num_cols):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.nodes_int = self._create_nodes()\n",
    "        \n",
    "        self.edges = set()\n",
    "        for row in range(self.num_rows): # Kinda like initialising a 2D matrix. From the nodes generated, add the edges to the set.\n",
    "            for col in range(self.num_cols):\n",
    "                node = self.nodes_mat[row][col]\n",
    "                \n",
    "                adj_list = self.mapper[node.val].adj_list\n",
    "                node = self.mapper[node.val]\n",
    "                for other_node in adj_list:\n",
    "                    self.edges.add((node, other_node))\n",
    "                    self.edges.add((other_node, node))\n",
    "        \n",
    "    def init_start_and_end(self):\n",
    "        self.start = 1 # top-left\n",
    "        self.end = self.num_rows * self.num_cols # bottom-right\n",
    "        \n",
    "    def _create_nodes(self):\n",
    "        nodes_int, i = [], 1\n",
    "        nodes = [[Node((row, col)) for col in range(self.num_cols)] for row in range(self.num_rows)] \n",
    "        mapper = dict()\n",
    "        int_mapper = dict()\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                node = Node(i) # Create the nodes, number 1 to (m * n).\n",
    "                nodes_int.append(node)\n",
    "                mapper[(row, col)] = node\n",
    "                int_mapper[i] = node\n",
    "                i += 1\n",
    "        \n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                node = nodes[row][col] # this stuff below is where we initialise the neighbours.\n",
    "                if row > 0:\n",
    "                    node.adj_list.add(nodes[row - 1][col])  # Upper neighbour.\n",
    "                    nodes[row - 1][col].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row - 1, col)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if row < self.num_rows - 1:\n",
    "                    node.adj_list.add(nodes[row + 1][col])  # Lower neighbour.\n",
    "                    nodes[row + 1][col].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row + 1, col)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if col > 0:\n",
    "                    node.adj_list.add(nodes[row][col - 1])  # Left neighbour.\n",
    "                    nodes[row][col - 1].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row, col - 1)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if col < self.num_cols - 1:\n",
    "                    node.adj_list.add(nodes[row][col + 1])  # Right neighbour.\n",
    "                    nodes[row][col + 1].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row, col + 1)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                \n",
    "        self.nodes_mat = nodes\n",
    "        self.mapper = mapper\n",
    "        self.int_mapper = int_mapper\n",
    "                    \n",
    "        return nodes_int\n",
    "        \n",
    "    def print_graph(self): # for debugging purposes\n",
    "        print([node.val for row in self.nodes_mat for node in row])\n",
    "        print()\n",
    "        print([node.val for node in self.nodes_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "id": "4c19ba43-13ce-467d-a7e1-d166c8fe6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class GameAI:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.m = self.graph.num_rows\n",
    "        self.n = self.graph.num_cols\n",
    "        \n",
    "        self.node_mapping = dict()\n",
    "        for i in range(1, (self.m * self.n) + 1):\n",
    "            self.node_mapping[i] = self.graph.nodes_int[i - 1] # e.g. 1 is in index 0, 2 is index 1, etc.\n",
    "        \n",
    "        self.edges = []\n",
    "        for i in range(1, (self.graph.num_rows * self.graph.num_cols) + 1):\n",
    "            adj_list = [node.val for node in self.graph.int_mapper[i].adj_list]\n",
    "            for adj_node in adj_list:\n",
    "                if (adj_node, i) in edges:\n",
    "                    continue\n",
    "                self.edges.append((i, adj_node))\n",
    "        \n",
    "        self.edges = sorted(edges)\n",
    "        \n",
    "        self.game = game\n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n # this is for the CUT player\n",
    "        self.secured = [] # this is what the FIX player chooses; nodes\n",
    "        self.secured_edges = [] # fix  \n",
    "        self.removed_edges = [] # cut\n",
    "        \n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    def reset(self): # Reset everything for the next training iteration.\n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n \n",
    "        self.secured = []\n",
    "        self.secured_edges = []\n",
    "        self.removed_edges = []\n",
    "        \n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    # This is what our AI will train against. Random shit.\n",
    "    def choose_edge_to_cut(self):\n",
    "        edge_to_cut = random.choice(list(self.remaining))\n",
    "        return edge_to_cut    \n",
    "    \n",
    "    # Plays step-by-step. This is what we'll use for \"learning\".\n",
    "    def next_step_player(self, chosen_edge):\n",
    "        if self.unsecured_count > 0:\n",
    "            if not self.end:\n",
    "                # 1. CUT/bot player's turn.\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    edge_to_cut = self.choose_edge_to_cut()\n",
    "                    self.cut(edge_to_cut)\n",
    "\n",
    "            if not self.end:\n",
    "                # 2. FIX player's turn: where the magic happens.\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    self.fix(chosen_edge)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "        else:\n",
    "            self.end = True\n",
    "\n",
    "    def is_fix_path_complete(self): # This does BFS to check if there is a path from the start to the end.\n",
    "        visited = set()\n",
    "        stack = [1]\n",
    "\n",
    "        while stack:\n",
    "            current_node = stack.pop()\n",
    "            if current_node == self.m * self.n: # e.g. 4x4, 16 is the bottom-right.\n",
    "                return True\n",
    "\n",
    "            if current_node not in visited:\n",
    "                visited.add(current_node)\n",
    "                adj_list = self.node_mapping[current_node].adj_list\n",
    "                \n",
    "                for nbr in adj_list:\n",
    "                    edge = (current_node, nbr.val)\n",
    "                    reverse_edge = (nbr.val, current_node)\n",
    "\n",
    "                    if edge in self.secured_edges or reverse_edge in self.secured_edges:\n",
    "                        if nbr.val not in visited:\n",
    "                            stack.append(nbr.val)\n",
    "\n",
    "        return False\n",
    "\n",
    "    # 1. CUT player's function; removes unsecured edge in question (and its reverse).\n",
    "    def cut(self, edge):\n",
    "        # edge = ex: (1, 4)\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.removed_edges.append(edge)\n",
    "            self.unsecured_count -= 1\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "\n",
    "    # 2. FIX player's function; secures unsecured edge in question (and its reverse).\n",
    "    def fix(self, edge):\n",
    "        # edge = ex: (1, 4)\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.secured.append(edge[0])\n",
    "            self.secured.append(edge[1])\n",
    "            self.secured_edges.append(edge)\n",
    "            self.unsecured_count -= 1\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "            \n",
    "    # Reward function\n",
    "    def get_reward(self):\n",
    "        if self.fix_win:\n",
    "            # Positive reward when the FIX player wins\n",
    "            reward = 1.0\n",
    "        elif self.end:\n",
    "            # Negative reward when the FIX player loses\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            # Intermediate reward for the ongoing game\n",
    "            reward = 0.0            \n",
    "\n",
    "        return reward\n",
    "            \n",
    "    def get_state(self):\n",
    "        # Define the state representation based on the game state.\n",
    "        secured_count = len(self.secured_edges)\n",
    "        remaining_count = int(len(self.remaining) / 2) # Because reverse edges are here too.\n",
    "        secured_edges = self.secured_edges\n",
    "        deleted_edges = self.removed_edges\n",
    "        remaining_edges = list(self.remaining) # Yet for this, we'll keep the reverse edges. Bit hypocritical, but fuck it.\n",
    "        \n",
    "        state = (secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count)\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    # This is still here purely for debugging purposes.\n",
    "    def play(self):\n",
    "        while self.unsecured_count > 0:\n",
    "            # 1. CUT player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "                break\n",
    "                \n",
    "            edge_to_cut = self.choose_edge_to_cut()\n",
    "            self.cut(edge_to_cut)\n",
    "        \n",
    "            # 2. FIX player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "                break\n",
    "\n",
    "            edge_to_fix = self.choose_edge_to_fix()\n",
    "            self.fix(edge_to_fix)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "                break\n",
    "            elif self.unsecured_count == 0:\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "\n",
    "    # This is still here purely for debugging purposes.\n",
    "    def choose_edge_to_fix(self):\n",
    "        edge_to_fix = random.choice(list(self.remaining))\n",
    "        return edge_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "id": "4aca2cef-6795-4345-97aa-7df444c7a0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7, 4), (8, 7), (8, 9), (4, 1)], [(2, 1), (5, 2), (2, 3), (5, 6)], True)"
      ]
     },
     "execution_count": 1475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph(3, 3)\n",
    "g.init_start_and_end()\n",
    "gameAI_test = GameAI(g)\n",
    "\n",
    "for i in range(10000):\n",
    "    gameAI_test.play()\n",
    "    if gameAI_test.fix_win and len(gameAI_test.secured_edges) == 4:\n",
    "        break\n",
    "        \n",
    "    gameAI_test.reset()\n",
    "    \n",
    "gameAI_test.secured_edges, gameAI_test.removed_edges, gameAI_test.fix_win"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9029d6-c623-4c99-8882-b04b46ea8731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "id": "06bde898-09a2-4a9b-8b50-7cb35778a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=L8ypSXwyBds\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "LR = 0.001\n",
    "MAX_MEMORY = 1_000_000\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "def convert_to_adj_matrix(edges, num_nodes):\n",
    "    nodes = set()\n",
    "    for edge in edges:\n",
    "        nodes.add(edge[0])\n",
    "        nodes.add(edge[1])\n",
    "\n",
    "    adj_matrix = np.zeros((num_nodes + 1, num_nodes + 1)) # 0th col and 0th row will just be to pad.\n",
    "\n",
    "    # Populate the adjacency matrix\n",
    "    for edge in edges:\n",
    "        adj_matrix[edge[0]][edge[1]] = 1\n",
    "        \n",
    "    return adj_matrix\n",
    "\n",
    "def find_max_number(lists_of_tuples):\n",
    "    max_number = float('-inf')\n",
    "\n",
    "    for list_of_tuples in lists_of_tuples:\n",
    "        for tup in list_of_tuples:\n",
    "            numbers = [x for x in tup if isinstance(x, (int, float))]\n",
    "            if numbers:\n",
    "                current_max = max(numbers)\n",
    "                if current_max > max_number:\n",
    "                    max_number = current_max\n",
    "\n",
    "    return max_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "id": "856e1e1a-7f03-4d49-884d-9da8b59315cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, game):\n",
    "        self.n_games = 0 # Iteration number\n",
    "        self.epsilon = 0 # Randomness for epsilon-greedy.\n",
    "        self.gamma = 0.9 # \"Discount rate\"\n",
    "        self.memory = deque(maxlen = MAX_MEMORY) # popleft()\n",
    "        \n",
    "        self.edges = []\n",
    "        for i in range(1, (game.graph.num_rows * game.graph.num_cols) + 1):\n",
    "            adj_list = [node.val for node in game.graph.int_mapper[i].adj_list]\n",
    "            for adj_node in adj_list:\n",
    "                if (adj_node, i) in edges:\n",
    "                    continue\n",
    "                self.edges.append((i, adj_node))\n",
    "        \n",
    "        self.edges = sorted(edges)\n",
    "        self.game = game\n",
    "        \n",
    "        # Secured edges not picked according to function in ShannonModel class.\n",
    "        self.model = ShannonModel(state_size, 302, (2 * game.m * game.n) - game.m - game.n, self.edges, game)\n",
    "        self.trainer = Trainer(self.model, lr = LR, gamma = self.gamma)\n",
    "        \n",
    "    # Used to clean up the game's state a little bit.\n",
    "    def get_state(self, game):\n",
    "        # (secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count)\n",
    "        state = game.get_state()\n",
    "        \n",
    "        num_nodes = find_max_number([state[0], state[1], state[2]])\n",
    "        \n",
    "        secured_edges = convert_to_adj_matrix(state[0], num_nodes)\n",
    "        deleted_edges = convert_to_adj_matrix(state[1], num_nodes)\n",
    "        remaining_edges = convert_to_adj_matrix(state[2], num_nodes)\n",
    "        secured_count, remaining_count = state[3], state[4]\n",
    "        \n",
    "        secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count = (torch.tensor(secured_edges).flatten(), \n",
    "                                                                                             torch.tensor(deleted_edges).flatten(), \n",
    "                                                                                             torch.tensor(remaining_edges).flatten(), \n",
    "                                                                                             torch.tensor([secured_count]), \n",
    "                                                                                             torch.tensor([remaining_count]))\n",
    "            \n",
    "        formatted_state = np.concatenate([secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count]).tolist()\n",
    "        formatted_state = torch.tensor(formatted_state)\n",
    "        \n",
    "        return formatted_state\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done): # Stores this shit into the deque so it can be used for training later.\n",
    "        # State must be from the formatted Agent.get_state()\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):        \n",
    "        # State must be from the formatted Agent.get_state()\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "        \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample) # From remember/memory\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # Exploration / exploitation.\n",
    "        # This epsilon will need a better threshold in the future.\n",
    "        self.epsilon = 80 - self.n_games # Hardcoded, can change this shit.\n",
    "        \n",
    "        remaining_edges = self.game.get_state()[2]\n",
    "        deleted_edges = self.game.get_state()[1]\n",
    "        secured_edges = self.game.get_state()[0]\n",
    "        num_nodes = find_max_number([remaining_edges, secured_edges, deleted_edges])\n",
    "        \n",
    "        remaining_edges = convert_to_adj_matrix(remaining_edges, num_nodes)\n",
    "        \n",
    "        if random.randint(0, 200) < self.epsilon: # This works.\n",
    "            indices = np.where(remaining_edges == 1) # This expects remaining_edges in the form of an adjacency matrix.\n",
    "            random_index = random.choice([i for i in range(len(indices[0]))]) \n",
    "            row_index, col_index = indices[0][random_index], indices[1][random_index]\n",
    "            final_edge = (row_index, col_index)\n",
    "        else:\n",
    "            prediction = self.model(state)\n",
    "            # print(prediction)\n",
    "            final_edge = prediction[-1]\n",
    "\n",
    "        return final_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "id": "b7cbcfce-8210-4103-8f67-57fa1ffbdee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 2),\n",
       "  (1, 4),\n",
       "  (2, 3),\n",
       "  (2, 5),\n",
       "  (3, 6),\n",
       "  (4, 5),\n",
       "  (4, 7),\n",
       "  (5, 6),\n",
       "  (5, 8),\n",
       "  (6, 9),\n",
       "  (7, 8),\n",
       "  (8, 9)],\n",
       " (7, 8))"
      ]
     },
     "execution_count": 1379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph(3, 3)\n",
    "g.init_start_and_end()\n",
    "gameAI = GameAI(g)\n",
    "agent = Agent(302, gameAI)\n",
    "boner = agent.get_action(agent.get_state(gameAI))\n",
    "agent.edges, boner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68caf67d-1c24-4841-af56-e0927ea4f756",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "id": "0da9238e-9490-4ce5-b088-eb4b8b5fbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model.py\n",
    "# 2. agent.py\n",
    "# model is the FFNN, agent is what trains the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "id": "42200eea-5064-4281-bef9-9cfa64f34e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Feedforward Neural Network.\n",
    "class ShannonModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, edges, game):\n",
    "        self.edges = edges\n",
    "        self.game = game\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        sorted_x, indices = torch.sort(x, descending = True)\n",
    "        max_probability = None\n",
    "        chosen_edge = None\n",
    "    \n",
    "        for index in indices:\n",
    "            edge = self.edges[index.item()]\n",
    "            \n",
    "            # Could be a problem here. Don't know what, but just troubleshooting for future.\n",
    "            if edge not in self.game.removed_edges and edge not in self.game.secured_edges: \n",
    "                max_probability = x[index]\n",
    "                chosen_edge = edge\n",
    "                break\n",
    "        \n",
    "        return x, max_probability, chosen_edge # Return the probabilities and the valid edge with the max probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1c052-e566-4cd4-8bfb-6ecdeff1a7c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1421,
   "id": "8140b657-78e9-48dd-9e5e-7237dc037943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This trains the ShannonModel() initialised above.\n",
    "class Trainer: \n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr # Learning Rate\n",
    "        self.gamma = gamma # https://ai.stackexchange.com/questions/8100/what-is-the-purpose-of-the-gamma-parameter-in-svms\n",
    "        self.model = model # ShannonModel()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = self.lr) # Adam Algorithm (some shit idek)\n",
    "        self.criterion = nn.MSELoss() # Standard MSE\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done): # Where the magic happens\n",
    "        # Needs to be implemented.\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Predictions for the current state and the next state.\n",
    "        pred = self.model(state)[0] # Q-values.\n",
    "        next_pred = self.model(next_state)[0]\n",
    "        \n",
    "        # Calculate the target value\n",
    "        target = pred.clone().detach()\n",
    "        max_next, _ = torch.max(next_pred, 1)        \n",
    "        target[action] = reward + self.gamma * max_next * (1 - done)\n",
    "    \n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "id": "e360b444-52cb-4157-9c37-15078c9dbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, game, episodes):\n",
    "    for episode in range(episodes):\n",
    "        game.reset()\n",
    "        state = agent.get_state(game)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Get the action from the agent\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Perform the action in the game\n",
    "            game.next_step_player(action) # action: e.g. edge (2, 3)\n",
    "            reward = game.get_reward() # Reward: -1.0, 0.0, or 1.0 (will be 0.0 majority of the time)\n",
    "            done = game.end # boolean\n",
    "\n",
    "            # Get the next state and remember the transition\n",
    "            next_state = agent.get_state(game) # concatenated matrix of size 302\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train the agent using short and long memory\n",
    "            agent.train_short_memory(state, action, reward, next_state, done)\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "        # Increment the number of games played by the agent\n",
    "        agent.n_games += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "id": "f1593655-4693-4033-b5e7-1e6e0755f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(3, 3)\n",
    "g.init_start_and_end()\n",
    "gameAI = GameAI(g)\n",
    "agent = Agent(302, gameAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc46a0-3275-4718-84c8-6e021799f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(agent, gameAI, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3952657-1189-484b-8e48-2e8ac0879bc1",
   "metadata": {},
   "source": [
    "## Training Attempt #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386f402-cbb2-4fd1-a9f0-b751cbc1b8b0",
   "metadata": {},
   "source": [
    "I'll just attempt to complete this without a Trainer or Agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "id": "66b96924-8ce6-466e-bec4-3bb950ec6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    num_nodes = find_max_number([state[0], state[1], state[2]])\n",
    "        \n",
    "    secured_edges = convert_to_adj_matrix(state[0], num_nodes)\n",
    "    deleted_edges = convert_to_adj_matrix(state[1], num_nodes)\n",
    "    remaining_edges = convert_to_adj_matrix(state[2], num_nodes)\n",
    "    secured_count, remaining_count = state[3], state[4]\n",
    "        \n",
    "    secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count = (torch.tensor(secured_edges).flatten(), \n",
    "                                                                                     torch.tensor(deleted_edges).flatten(), \n",
    "                                                                                     torch.tensor(remaining_edges).flatten(), \n",
    "                                                                                     torch.tensor([secured_count]), \n",
    "                                                                                     torch.tensor([remaining_count]))\n",
    "            \n",
    "    formatted_state = np.concatenate([secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count]).tolist()\n",
    "    formatted_state = torch.tensor(formatted_state)\n",
    "    \n",
    "    return formatted_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "id": "883ae747-7ba7-4040-bc39-59d2d969e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph, game .. \n",
    "graph = Graph(num_rows = 3, num_cols = 3)\n",
    "game = GameAI(graph)\n",
    "\n",
    "# (set up hyperparameters and other constants)\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "input_size = 302 # Secured edges, deleted edges, remaining edges, secured count, remaining count.\n",
    "hidden_size = 256\n",
    "output_size = (2 * game.m * game.n) - game.m - game.n # Number of possible actions.\n",
    "\n",
    "# .. and model.\n",
    "model = ShannonModel(input_size, hidden_size, output_size, game.edges, game)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd4343-e0e1-4180-87ff-8fabce7189d3",
   "metadata": {},
   "source": [
    "Pain in the neck. The other notebooks are good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
