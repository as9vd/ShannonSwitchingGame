{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f4de6d-969d-430a-bf51-5c71b6f585b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Node and Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07e9a0ec-88c0-43e8-9db3-361b7654124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, val):\n",
    "        self.val = val # Value of node = its position (left, right, zigzag, continue).\n",
    "        self.adj_list = set()\n",
    "\n",
    "# create a list of edges for Game\n",
    "class Graph:\n",
    "    def __init__(self, num_rows, num_cols):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.nodes_int = self._create_nodes()\n",
    "        \n",
    "        self.edges = set()\n",
    "        for row in range(self.num_rows): # Kinda like initialising a 2D matrix. From the nodes generated, add the edges to the set.\n",
    "            for col in range(self.num_cols):\n",
    "                node = self.nodes_mat[row][col]\n",
    "                \n",
    "                adj_list = self.mapper[node.val].adj_list\n",
    "                node = self.mapper[node.val]\n",
    "                for other_node in adj_list:\n",
    "                    self.edges.add((node, other_node))\n",
    "                    self.edges.add((other_node, node))\n",
    "        \n",
    "    def init_start_and_end(self):\n",
    "        self.start = 1 # top-left\n",
    "        self.end = self.num_rows * self.num_cols # bottom-right\n",
    "        \n",
    "    def _create_nodes(self):\n",
    "        nodes_int, i = [], 1\n",
    "        nodes = [[Node((row, col)) for col in range(self.num_cols)] for row in range(self.num_rows)] \n",
    "        mapper = dict()\n",
    "        int_mapper = dict()\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                node = Node(i) # Create the nodes, number 1 to (m * n).\n",
    "                nodes_int.append(node)\n",
    "                mapper[(row, col)] = node\n",
    "                int_mapper[i] = node\n",
    "                i += 1\n",
    "        \n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                node = nodes[row][col] # this stuff below is where we initialise the neighbours.\n",
    "                if row > 0:\n",
    "                    node.adj_list.add(nodes[row - 1][col])  # Upper neighbour.\n",
    "                    nodes[row - 1][col].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row - 1, col)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if row < self.num_rows - 1:\n",
    "                    node.adj_list.add(nodes[row + 1][col])  # Lower neighbour.\n",
    "                    nodes[row + 1][col].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row + 1, col)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if col > 0:\n",
    "                    node.adj_list.add(nodes[row][col - 1])  # Left neighbour.\n",
    "                    nodes[row][col - 1].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row, col - 1)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                    \n",
    "                if col < self.num_cols - 1:\n",
    "                    node.adj_list.add(nodes[row][col + 1])  # Right neighbour.\n",
    "                    nodes[row][col + 1].adj_list.add(node)\n",
    "                    \n",
    "                    current_mapping = mapper[(row, col)]\n",
    "                    nbr_mapping = mapper[(row, col + 1)]\n",
    "                    \n",
    "                    current_mapping.adj_list.add(nbr_mapping)\n",
    "                    nbr_mapping.adj_list.add(current_mapping)\n",
    "                \n",
    "        self.nodes_mat = nodes\n",
    "        self.mapper = mapper\n",
    "        self.int_mapper = int_mapper\n",
    "                    \n",
    "        return nodes_int\n",
    "        \n",
    "    def print_graph(self): # for debugging purposes\n",
    "        print([node.val for row in self.nodes_mat for node in row])\n",
    "        print()\n",
    "        print([node.val for node in self.nodes_int])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a396c4a-9076-4996-8736-187dcec31000",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## GameAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7b04c-3fec-4b39-a9d5-3afd6dd7b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class GameAI:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.m = self.graph.num_rows\n",
    "        self.n = self.graph.num_cols\n",
    "        \n",
    "        self.node_mapping = dict()\n",
    "        for i in range(1, (self.m * self.n) + 1):\n",
    "            self.node_mapping[i] = self.graph.nodes_int[i - 1] # e.g. 1 is in index 0, 2 is index 1, etc.\n",
    "        \n",
    "        self.edges = []\n",
    "        for i in range(1, (self.graph.num_rows * self.graph.num_cols) + 1):\n",
    "            adj_list = [node.val for node in self.graph.int_mapper[i].adj_list]\n",
    "            for adj_node in adj_list:\n",
    "                if (adj_node, i) in self.edges:\n",
    "                    continue\n",
    "                self.edges.append((i, adj_node))\n",
    "        \n",
    "        self.edges = sorted(self.edges)\n",
    "        \n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n # this is for the CUT player\n",
    "        self.secured_edges = [] # fix  \n",
    "        self.removed_edges = [] # cut\n",
    "        \n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    def reset(self): # Reset everything for the next training iteration.\n",
    "        self.unsecured_count = (2 * self.m * self.n) - self.m - self.n \n",
    "        self.secured_edges = []\n",
    "        self.removed_edges = []\n",
    "        \n",
    "        self.remaining = {(node1.val, node2.val) for node1, node2 in self.graph.edges} \n",
    "        self.fix_win = False\n",
    "        self.end = False\n",
    "        \n",
    "    # This is what our AI will train against. Random shit.\n",
    "    def choose_edge_to_cut(self):\n",
    "        edge_to_cut = random.choice(list(self.remaining))\n",
    "        return edge_to_cut    \n",
    "    \n",
    "    # Plays step-by-step. This is what we'll use for \"learning\".\n",
    "    def next_step_player(self, chosen_edge):\n",
    "        if self.unsecured_count > 0 and not self.end:\n",
    "            if not self.end:\n",
    "                # 1. FIX player's turn: where the magic happens. (HE NOW GOES FIRST)\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    self.fix(chosen_edge)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "                return\n",
    "                \n",
    "            # 2. CUT/bot player's turn.\n",
    "            if not self.end:\n",
    "                if len(self.remaining) == 0:\n",
    "                    # No more valid edges to choose.\n",
    "                    self.fix_win = False\n",
    "                    self.end = True\n",
    "                else:\n",
    "                    edge_to_cut = self.choose_edge_to_cut()\n",
    "                    self.cut(edge_to_cut)\n",
    "                    \n",
    "            if len(self.remaining) == 0:\n",
    "                self.end = True\n",
    "        else:\n",
    "            self.end = True\n",
    "\n",
    "    def is_fix_path_complete(self): # This does BFS to check if there is a path from the start to the end.\n",
    "        visited = set()\n",
    "        stack = [1]\n",
    "\n",
    "        while stack:\n",
    "            current_node = stack.pop()\n",
    "            if current_node == self.m * self.n: # e.g. 4x4, 16 is the bottom-right.\n",
    "                return True\n",
    "\n",
    "            if current_node not in visited:\n",
    "                visited.add(current_node)\n",
    "                adj_list = self.node_mapping[current_node].adj_list\n",
    "                \n",
    "                for nbr in adj_list:\n",
    "                    edge = (current_node, nbr.val)\n",
    "                    reverse_edge = (nbr.val, current_node)\n",
    "\n",
    "                    if edge in self.secured_edges or reverse_edge in self.secured_edges:\n",
    "                        if nbr.val not in visited:\n",
    "                            stack.append(nbr.val)\n",
    "\n",
    "        return False\n",
    "    \n",
    "    # 1. FIX player's function; secures unsecured edge in question (and its reverse).\n",
    "    def fix(self, edge):\n",
    "        # edge = ex: (1, 4)\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.secured_edges.append(edge)\n",
    "            self.unsecured_count -= 1\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "\n",
    "    # 2. CUT player's function; removes unsecured edge in question (and its reverse).\n",
    "    def cut(self, edge):\n",
    "        # edge = ex: (1, 4)\n",
    "        if edge in self.remaining:\n",
    "            self.remaining.remove(edge)\n",
    "            self.removed_edges.append(edge)\n",
    "            self.unsecured_count -= 1\n",
    "            \n",
    "        # Also remove the reverse direction of the edge.\n",
    "        reverse_edge = (edge[1], edge[0])\n",
    "        if reverse_edge in self.remaining:\n",
    "            self.remaining.remove(reverse_edge)\n",
    "  \n",
    "    # Reward function\n",
    "    def get_reward(self):\n",
    "        if self.fix_win:\n",
    "            # Positive reward when the FIX player wins\n",
    "            reward = 1.0\n",
    "        elif self.end:\n",
    "            # Negative reward when the FIX player loses\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            # Intermediate reward for the ongoing game\n",
    "            reward = -0.1            \n",
    "\n",
    "        return reward\n",
    "            \n",
    "    def get_state(self):\n",
    "        # Define the state representation based on the game state.\n",
    "        secured_count = len(self.secured_edges)\n",
    "        remaining_count = self.unsecured_count\n",
    "        secured_edges = self.secured_edges\n",
    "        deleted_edges = self.removed_edges\n",
    "        remaining_edges = list(self.remaining) # Yet for this, we'll keep the reverse edges. Bit hypocritical, but fuck it.\n",
    "        \n",
    "        state = (secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count)\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    # This is still here purely for debugging purposes.\n",
    "    def play(self):\n",
    "        while self.unsecured_count > 0:\n",
    "            # 1. CUT player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "                break\n",
    "                \n",
    "            edge_to_cut = self.choose_edge_to_cut()\n",
    "            self.cut(edge_to_cut)\n",
    "        \n",
    "            # 2. FIX player's turn\n",
    "            if len(self.remaining) == 0:\n",
    "                # No more valid edges to choose.\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "\n",
    "            edge_to_fix = self.choose_edge_to_fix()\n",
    "            self.fix(edge_to_fix)\n",
    "            \n",
    "            if self.is_fix_path_complete():\n",
    "                self.fix_win = True\n",
    "                self.end = True\n",
    "                break\n",
    "            elif self.unsecured_count == 0:\n",
    "                self.fix_win = False\n",
    "                self.end = True\n",
    "\n",
    "    # This is still here purely for debugging purposes.\n",
    "    def choose_edge_to_fix(self):\n",
    "        edge_to_fix = random.choice(list(self.remaining))\n",
    "        return edge_to_fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a5c02-68b8-4d8f-b4fd-f062a7ef4e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "81054043-5201-470e-8b17-dac2c3aa61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. model.py\n",
    "# 2. agent.py\n",
    "# model is the FFNN, agent is what trains the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# This is the Feedforward Neural Network.\n",
    "class ShannonModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, edges, game):\n",
    "        self.edges = edges\n",
    "        self.game = game\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        sorted_x, indices = torch.sort(x, descending = True)\n",
    "        max_probability = None\n",
    "        chosen_edge = None\n",
    "    \n",
    "        for index in indices:\n",
    "            edge = self.edges[index.item()]\n",
    "            reverse_edge = (edge[1], edge[0])\n",
    "            \n",
    "            # Could be a problem here. Don't know what, but just troubleshooting for future.\n",
    "            if ((edge not in self.game.removed_edges and edge not in self.game.secured_edges) and \n",
    "                (reverse_edge not in self.game.removed_edges and reverse_edge not in self.game.secured_edges)): \n",
    "                max_probability = x[index]\n",
    "                chosen_edge = edge\n",
    "                break\n",
    "                \n",
    "        return x, max_probability, chosen_edge # Return the probabilities and the valid edge with the max probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f24d5-3892-48e1-9174-4290412836f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "f8edbac2-da83-408e-a90f-9244cda27360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def format_state(state):\n",
    "    num_nodes = find_max_number([state[0], state[1], state[2]])\n",
    "        \n",
    "    secured_edges = convert_to_adj_matrix(state[0], num_nodes)\n",
    "    deleted_edges = convert_to_adj_matrix(state[1], num_nodes)\n",
    "    remaining_edges = convert_to_adj_matrix(state[2], num_nodes)\n",
    "    secured_count, remaining_count = state[3], state[4]\n",
    "        \n",
    "    secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count = (torch.tensor(secured_edges).flatten(), \n",
    "                                                                                     torch.tensor(deleted_edges).flatten(), \n",
    "                                                                                     torch.tensor(remaining_edges).flatten(), \n",
    "                                                                                     torch.tensor([secured_count]), \n",
    "                                                                                     torch.tensor([remaining_count]))\n",
    "            \n",
    "    formatted_state = np.concatenate([secured_edges, deleted_edges, remaining_edges, secured_count, remaining_count]).tolist()\n",
    "    formatted_state = torch.tensor(formatted_state)\n",
    "    \n",
    "    return formatted_state\n",
    "\n",
    "def convert_to_adj_matrix(edges, num_nodes):\n",
    "    nodes = set()\n",
    "    for edge in edges:\n",
    "        nodes.add(edge[0])\n",
    "        nodes.add(edge[1])\n",
    "\n",
    "    adj_matrix = np.zeros((num_nodes + 1, num_nodes + 1)) # 0th col and 0th row will just be to pad.\n",
    "\n",
    "    # Populate the adjacency matrix\n",
    "    for edge in edges:\n",
    "        adj_matrix[edge[0]][edge[1]] = 1\n",
    "        \n",
    "    return adj_matrix\n",
    "\n",
    "def find_max_number(lists_of_tuples):\n",
    "    max_number = float('-inf')\n",
    "\n",
    "    for list_of_tuples in lists_of_tuples:\n",
    "        for tup in list_of_tuples:\n",
    "            numbers = [x for x in tup if isinstance(x, (int, float))]\n",
    "            if numbers:\n",
    "                current_max = max(numbers)\n",
    "                if current_max > max_number:\n",
    "                    max_number = current_max\n",
    "\n",
    "    return max_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "8a8c9d0a-c1c5-401a-b5f1-6d4dc98ee772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4096, Loss: 0.0013\n",
      "Epoch: 257/4096, Loss: 0.0000\n",
      "Epoch: 513/4096, Loss: 0.0002\n",
      "Epoch: 769/4096, Loss: 0.0001\n",
      "Epoch: 1025/4096, Loss: 0.0000\n",
      "Epoch: 1281/4096, Loss: 0.0000\n",
      "Epoch: 1537/4096, Loss: 0.0901\n",
      "Epoch: 1793/4096, Loss: 0.0000\n",
      "Epoch: 2049/4096, Loss: 0.0000\n",
      "Epoch: 2305/4096, Loss: 0.0000\n",
      "Epoch: 2561/4096, Loss: 0.0000\n",
      "Epoch: 2817/4096, Loss: 0.0000\n",
      "Epoch: 3073/4096, Loss: 0.0000\n",
      "Epoch: 3329/4096, Loss: 0.0000\n",
      "Epoch: 3585/4096, Loss: 0.0000\n",
      "Epoch: 3841/4096, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Parameters.\n",
    "num_epochs = 64 * 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = 869\n",
    "hidden_size = 256\n",
    "output_size = 24\n",
    "gamma = 0.90\n",
    "target_update = 256\n",
    "\n",
    "# Create the Graph and GameAI instances.\n",
    "num_rows = 4\n",
    "num_cols = 4\n",
    "graph = Graph(num_rows, num_cols)\n",
    "game = GameAI(graph)\n",
    "edges = game.edges\n",
    "\n",
    "# Initialize the Feedforward Neural Network (policy network) and target network.\n",
    "policy_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    next_state_list = []\n",
    "    done_list = []\n",
    "\n",
    "    while not game.end:\n",
    "        # Get the current state\n",
    "        state = game.get_state()\n",
    "        formatted_state = format_state(state)\n",
    "\n",
    "        # Choose an action using the policy network.\n",
    "        _, _, action = policy_net(formatted_state)\n",
    "\n",
    "        # Perform the action and get the reward\n",
    "        game.next_step_player(action)\n",
    "        reward = game.get_reward()\n",
    "        next_state = game.get_state()\n",
    "        formatted_next_state = format_state(next_state)\n",
    "        done = game.end\n",
    "\n",
    "        # Store the transition\n",
    "        state_list.append(formatted_state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        next_state_list.append(formatted_next_state)\n",
    "        done_list.append(done)\n",
    "                \n",
    "    # Reset the game if it has ended\n",
    "    game.reset()\n",
    "\n",
    "    # Create a DataLoader for the collected data\n",
    "    dataset = TensorDataset(torch.stack(state_list), \n",
    "                            torch.tensor(action_list), \n",
    "                            torch.tensor(reward_list, dtype=torch.float32), \n",
    "                            torch.stack(next_state_list), \n",
    "                            torch.tensor(done_list))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the policy network\n",
    "    for batch_idx, (states, actions, rewards, next_states, dones) in enumerate(dataloader):\n",
    "        for i in range(len(states)):           \n",
    "            q_values, max_q, chosen_edge = policy_net(states[i])\n",
    "            next_q_values, max_next_q, chosen_next_edge = target_net(next_states[i])\n",
    "            \n",
    "            # target_q_values = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "            target_q_values = q_values.clone().detach()\n",
    "            target_q_values[actions[i]] = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "\n",
    "            loss = criterion(q_values, target_q_values.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update the target network\n",
    "    if epoch % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "0e55cbd5-6bca-4202-b8e2-c47a3e5a4f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "game.reset()\n",
    "while not game.end:\n",
    "    state = format_state(game.get_state())\n",
    "    _, _, chosen_edge = policy_net(state)\n",
    "    game.next_step_player(chosen_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "ef11d010-261e-421e-95bc-0cef89099d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train_wins, train_wins = 0, 0\n",
    "for i in range(1000):\n",
    "    game.reset()\n",
    "    game.play()\n",
    "        \n",
    "    if game.fix_win:\n",
    "        no_train_wins += 1\n",
    "        \n",
    "for i in range(1000):\n",
    "    game.reset()\n",
    "    while not game.end:\n",
    "        state = format_state(game.get_state())\n",
    "        _, _, chosen_edge = policy_net(state)\n",
    "        game.next_step_player(chosen_edge)\n",
    "        \n",
    "    if game.fix_win:\n",
    "        train_wins += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "f5c4d676-82a6-4a5b-a319-af4edd3908a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 249)"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_train_wins, train_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37044dbf-82e8-4bcd-9a90-5dc536341380",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7x7 Beloved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "fee30590-83b3-4efc-a0a5-2cb237a7bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2048, Loss: 0.0006\n",
      "Epoch: 129/2048, Loss: 0.0000\n",
      "Epoch: 257/2048, Loss: 0.0000\n",
      "Epoch: 385/2048, Loss: 0.0000\n",
      "Epoch: 513/2048, Loss: 0.0000\n",
      "Epoch: 641/2048, Loss: 0.0000\n",
      "Epoch: 769/2048, Loss: 0.0000\n",
      "Epoch: 897/2048, Loss: 0.0029\n",
      "Epoch: 1025/2048, Loss: 0.0000\n",
      "Epoch: 1153/2048, Loss: 0.0000\n",
      "Epoch: 1281/2048, Loss: 0.0000\n",
      "Epoch: 1409/2048, Loss: 0.0000\n",
      "Epoch: 1537/2048, Loss: 0.0000\n",
      "Epoch: 1665/2048, Loss: 0.0000\n",
      "Epoch: 1793/2048, Loss: 0.0000\n",
      "Epoch: 1921/2048, Loss: 0.0246\n"
     ]
    }
   ],
   "source": [
    "# Parameters.\n",
    "num_epochs = 64 * 32\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = 7502\n",
    "hidden_size = 256\n",
    "output_size = 84\n",
    "gamma = 0.90\n",
    "target_update = 128\n",
    "\n",
    "# Create the Graph and GameAI instances.\n",
    "num_rows = 7\n",
    "num_cols = 7\n",
    "graph = Graph(num_rows, num_cols)\n",
    "game = GameAI(graph)\n",
    "edges = game.edges\n",
    "\n",
    "# Initialize the Feedforward Neural Network (policy network) and target network.\n",
    "policy_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    next_state_list = []\n",
    "    done_list = []\n",
    "\n",
    "    while not game.end:\n",
    "        # Get the current state\n",
    "        state = game.get_state()\n",
    "        formatted_state = format_state(state)\n",
    "\n",
    "        # Choose an action using the policy network.\n",
    "        _, _, action = policy_net(formatted_state)\n",
    "\n",
    "        # Perform the action and get the reward\n",
    "        game.next_step_player(action)\n",
    "        reward = game.get_reward()\n",
    "        next_state = game.get_state()\n",
    "        formatted_next_state = format_state(next_state)\n",
    "        done = game.end\n",
    "\n",
    "        # Store the transition\n",
    "        state_list.append(formatted_state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        next_state_list.append(formatted_next_state)\n",
    "        done_list.append(done)\n",
    "                \n",
    "    # Reset the game if it has ended\n",
    "    game.reset()\n",
    "\n",
    "    # Create a DataLoader for the collected data\n",
    "    dataset = TensorDataset(torch.stack(state_list), \n",
    "                            torch.tensor(action_list), \n",
    "                            torch.tensor(reward_list, dtype=torch.float32), \n",
    "                            torch.stack(next_state_list), \n",
    "                            torch.tensor(done_list))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the policy network\n",
    "    for batch_idx, (states, actions, rewards, next_states, dones) in enumerate(dataloader):\n",
    "        for i in range(len(states)):           \n",
    "            q_values, max_q, chosen_edge = policy_net(states[i])\n",
    "            next_q_values, max_next_q, chosen_next_edge = target_net(next_states[i])\n",
    "            \n",
    "            # target_q_values = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "            target_q_values = q_values.clone().detach()\n",
    "            target_q_values[actions[i]] = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "\n",
    "            loss = criterion(q_values, target_q_values.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update the target network\n",
    "    if epoch % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "a1a8b486-aaef-4aef-8bf3-3ac276bc8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"7x7Model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "5c94f781-4ebc-47e7-ab59-8de8b315d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_comparisons = []\n",
    "for x in range(10):\n",
    "    no_train_wins, train_wins = 0, 0\n",
    "    for i in range(1000):\n",
    "        game.reset()\n",
    "        game.play()\n",
    "        \n",
    "        if game.fix_win:\n",
    "            no_train_wins += 1\n",
    "        \n",
    "    for i in range(1000):\n",
    "        game.reset()\n",
    "        while not game.end:\n",
    "            state = format_state(game.get_state())\n",
    "            _, _, chosen_edge = policy_net(state)\n",
    "            game.next_step_player(chosen_edge)\n",
    "        \n",
    "        if game.fix_win:\n",
    "            train_wins += 1\n",
    "            \n",
    "    overall_comparisons.append((no_train_wins, train_wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "50dc6cda-0c83-4ba8-ba28-2c638f0904dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76, 218),\n",
       " (64, 203),\n",
       " (71, 229),\n",
       " (64, 224),\n",
       " (53, 216),\n",
       " (76, 217),\n",
       " (70, 199),\n",
       " (55, 215),\n",
       " (67, 223),\n",
       " (64, 203)]"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "a26dbaa0-5ed8-4399-a2db-7ff6240d715a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 2147)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_wins, model_wins = 0, 0\n",
    "for tup in overall_comparisons:\n",
    "    random_w, model_w = tup\n",
    "    random_wins += random_w\n",
    "    model_wins += model_w\n",
    "    \n",
    "random_wins, model_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc11c5b-0074-4e55-9452-ea7c032a81a8",
   "metadata": {},
   "source": [
    "#### In 20000 different games, the trained AI won nearly 1500 more matches than the AI picking edges at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed031c-c2a3-46b4-839a-fea6b946d66e",
   "metadata": {},
   "source": [
    "## 10x10 Beloved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "0fae46a5-698d-49ac-98c5-17122c622561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/128, Loss: 0.0000\n",
      "Epoch: 17/128, Loss: 0.0000\n",
      "Epoch: 33/128, Loss: 0.0000\n",
      "Epoch: 49/128, Loss: 0.0000\n",
      "Epoch: 65/128, Loss: 0.0001\n",
      "Epoch: 81/128, Loss: 0.0000\n",
      "Epoch: 97/128, Loss: 0.0000\n",
      "Epoch: 113/128, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Parameters.\n",
    "num_epochs = 128\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "input_size = 30605\n",
    "hidden_size = 512\n",
    "output_size = 180\n",
    "gamma = 0.90\n",
    "target_update = 16\n",
    "\n",
    "# Create the Graph and GameAI instances.\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "graph = Graph(num_rows, num_cols)\n",
    "game = GameAI(graph)\n",
    "edges = game.edges\n",
    "\n",
    "# Initialize the Feedforward Neural Network (policy network) and target network.\n",
    "policy_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net = ShannonModel(input_size, hidden_size, output_size, edges, game)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    next_state_list = []\n",
    "    done_list = []\n",
    "\n",
    "    while not game.end:\n",
    "        # Get the current state\n",
    "        state = game.get_state()\n",
    "        formatted_state = format_state(state)\n",
    "\n",
    "        # Choose an action using the policy network.\n",
    "        _, _, action = policy_net(formatted_state)\n",
    "\n",
    "        # Perform the action and get the reward\n",
    "        game.next_step_player(action)\n",
    "        reward = game.get_reward()\n",
    "        next_state = game.get_state()\n",
    "        formatted_next_state = format_state(next_state)\n",
    "        done = game.end\n",
    "\n",
    "        # Store the transition\n",
    "        state_list.append(formatted_state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        next_state_list.append(formatted_next_state)\n",
    "        done_list.append(done)\n",
    "                \n",
    "    # Reset the game if it has ended\n",
    "    game.reset()\n",
    "\n",
    "    # Create a DataLoader for the collected data\n",
    "    dataset = TensorDataset(torch.stack(state_list), \n",
    "                            torch.tensor(action_list), \n",
    "                            torch.tensor(reward_list, dtype=torch.float32), \n",
    "                            torch.stack(next_state_list), \n",
    "                            torch.tensor(done_list))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the policy network\n",
    "    for batch_idx, (states, actions, rewards, next_states, dones) in enumerate(dataloader):\n",
    "        for i in range(len(states)):           \n",
    "            q_values, max_q, chosen_edge = policy_net(states[i])\n",
    "            next_q_values, max_next_q, chosen_next_edge = target_net(next_states[i])\n",
    "            \n",
    "            # target_q_values = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "            target_q_values = q_values.clone().detach()\n",
    "            target_q_values[actions[i]] = rewards[i] + (1 - dones[i].float()) * gamma * next_q_values.max()\n",
    "\n",
    "            loss = criterion(q_values, target_q_values.detach())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update the target network\n",
    "    if epoch % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "ce5c8aba-613b-41a1-a0e7-e1d6c1d75c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"10x10Model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "6919df9b-4ceb-40ed-95c0-49d5eac58357",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.reset()\n",
    "while not game.end:\n",
    "    state = format_state(game.get_state())\n",
    "    _, _, chosen_edge = policy_net(state)\n",
    "    game.next_step_player(chosen_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "f88c9688-0454-4f8e-b810-e074179c09ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(74, 75),\n",
       " (79, 89),\n",
       " (77, 78),\n",
       " (56, 57),\n",
       " (94, 95),\n",
       " (70, 80),\n",
       " (86, 87),\n",
       " (65, 75),\n",
       " (61, 62),\n",
       " (96, 97),\n",
       " (54, 64),\n",
       " (55, 65),\n",
       " (68, 69),\n",
       " (89, 99),\n",
       " (97, 98),\n",
       " (88, 89),\n",
       " (61, 71),\n",
       " (58, 59),\n",
       " (87, 97),\n",
       " (71, 72),\n",
       " (93, 94),\n",
       " (75, 85),\n",
       " (85, 86),\n",
       " (64, 74),\n",
       " (98, 99),\n",
       " (76, 77),\n",
       " (63, 64),\n",
       " (69, 79),\n",
       " (91, 92),\n",
       " (57, 58),\n",
       " (84, 94),\n",
       " (92, 93),\n",
       " (72, 73),\n",
       " (76, 86),\n",
       " (74, 84),\n",
       " (68, 78),\n",
       " (56, 66),\n",
       " (73, 74),\n",
       " (78, 88),\n",
       " (82, 83),\n",
       " (62, 63),\n",
       " (83, 84),\n",
       " (80, 90),\n",
       " (95, 96),\n",
       " (55, 56),\n",
       " (57, 67),\n",
       " (77, 87),\n",
       " (85, 95),\n",
       " (73, 83),\n",
       " (79, 80),\n",
       " (63, 73),\n",
       " (58, 68),\n",
       " (84, 85),\n",
       " (45, 46),\n",
       " (89, 90),\n",
       " (39, 49),\n",
       " (52, 62),\n",
       " (41, 42),\n",
       " (26, 36),\n",
       " (31, 32),\n",
       " (18, 19),\n",
       " (53, 63),\n",
       " (29, 39),\n",
       " (27, 28),\n",
       " (49, 59),\n",
       " (2, 3),\n",
       " (21, 31),\n",
       " (25, 35),\n",
       " (10, 20),\n",
       " (50, 60),\n",
       " (44, 54),\n",
       " (19, 29),\n",
       " (48, 49),\n",
       " (35, 36),\n",
       " (34, 44),\n",
       " (46, 47),\n",
       " (13, 14),\n",
       " (47, 48),\n",
       " (46, 56),\n",
       " (20, 30),\n",
       " (39, 40),\n",
       " (36, 46),\n",
       " (29, 30),\n",
       " (37, 38),\n",
       " (86, 96),\n",
       " (24, 34),\n",
       " (6, 16),\n",
       " (71, 81),\n",
       " (32, 42),\n",
       " (11, 21)]"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.secured_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "62cbbeea-5c35-4610-b224-edc6b6cc9b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_comparisons = []\n",
    "no_train_wins, train_wins = 0, 0\n",
    "for i in range(1000):\n",
    "    game.reset()\n",
    "    game.play()\n",
    "        \n",
    "    if game.fix_win:\n",
    "        no_train_wins += 1\n",
    "        \n",
    "for i in range(1000):\n",
    "    game.reset()\n",
    "    while not game.end:\n",
    "        state = format_state(game.get_state())\n",
    "        _, _, chosen_edge = policy_net(state)\n",
    "        game.next_step_player(chosen_edge)\n",
    "        \n",
    "    if game.fix_win:\n",
    "        train_wins += 1\n",
    "            \n",
    "overall_comparisons.append((no_train_wins, train_wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "c6271a13-c04f-4b8a-a0a6-528ee534392e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 1)"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_train_wins, train_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f776043-1759-46fa-b29b-92eb50f17d75",
   "metadata": {},
   "source": [
    "#### The 10x10 model clearly proved to be a bit crap.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
